### 课程内容

视频编号：P29 - P34

1. 文本的表示
2. 文本的相似度
3. TF-IDF
4. 词向量
5. 倒排表

### 文本的表示

**单词的表示**

表示单词的方法一般有两种：One-Hot，词向量。这里先讲解一下什么是One-Hot

假设我们已经有了一个词典 $V =  [w_1, w_2, w_3, w_4, ... , w_n]$，其长度写作 $ n = |V|$ 。

那么，对于词 $w_1$ 就可以写成 $w_1 = (1, 0, 0, ... , 0)$ ，同理 $ w_2 = (0, 1, 0, ... , 0)$。对于每一个单词来说，都是一个One-Hot的向量，而且有 $ n = |w_1| = |w_2|$。可看出，用One-Hot表示的词，词的长度与词典的长度相同。

**句子的表示**

假设词典  $V =  [w_1, w_2, w_3, w_4, ... , w_n]$ $， n = |V|$ 

若句子 $S_1 = w_1w_2w_3$，那么句子可以写作 $S_1 = (1,1,1,0,...,0)$，同理，若句子$S_2 = w_1w_3w_n$，那么$S_2$ 就可以写作$S_2 = (1,0,1,0,...,1)$。这时候，如果句子中某个词出现了多次，那么在该词所对应的位置上，也是1。这里的1和0表示存在于不存在。

对于一个句子，除了One-Hot表示法以外，还有基于Count的表示方法。它的形式与基于One-Hot的方法其实一模一样。不过只有一点不一样，就是对于一个出现多次的单词，它不再用1表示，而是用出现的次数来表示。比如说一个句子$S_3 = w_1w_1w_1w_2w_2w_3$，那么这个句子在基于Count的方法下可以写作$S_3 = (3,2,1,0,...0)$。

### 文本的相似度

句子之间的相似度用两种计算方式：欧氏距离和余弦相似度。

欧氏距离就是 $d = |S_1 - S_2|$，距离越近的句子相似度越高。

若 $S_1 = (x_1,x_2,x_3), S_2 = (y_1,y_2,y_3)$，那么
$$
d = \sqrt{(x_1-y_1)^2-(x_2-y_2)^2+(x_3-y_3)^2}
$$
余弦相似度就是:
$$
d = \frac{S_1·S_2}{|S_1|*|S_2|}
$$
由于句子是一个向量，是存在方向的，显然欧氏距离的缺点就是无法根据方向来考虑句子之间的相似情况。而余弦相似度则可以计算出方向上的差距。对于余弦相似度而言，$d$ 越大，相似度越大。而欧氏距离则相反。

### TF-IDF

对于一个基于Count表示的句子来说，一个词出现的频率越高，他一定重要吗？这个问题显然可以回答说，出现频率越高，不一定重要。所以要表示一个句子，基于Count的方法是有缺陷的。

我们知道：

- 频率大的词不一定重要
- 频率小的词不一定不重要

为了解决这个情况，TF-IDF (Term Frequency - Inversed Document Frequency)应运而生：
$$
tf-idf(w) = tf(d,w) * idf(w)
\\
idf(w) = \log\frac{N}{N(w)}
\\
w: 单词
\\
d:单词所在的文档(句子)
\\
N:文档总数
\\
N(w):w出现在多少个文档
$$
不难发现，其实 $idf(w)$ 这一项衡量的是单词的重要性，单词在不同的文档中出现的次数越多，$N(w)$的值会越大，而 $idf(w)$ 的值就会越小。

### 词向量

无论是普通的One-Hot，还是基于Count的方法，亦或是TF-IDF方法，都有致命的缺点，那就是无法表示词与词之间语义的相似度，而且One-Hot这一个大类的方法所表示出的词向量都存在严重的稀疏性的问题。一个句子的长度与词典的长度相等，但是里面却有很多很多的零。

为了解决这语义和稀疏性的问题，可以用分布式的表示方法 (Distributed Representation)

分布式的优点是：自定义句子长度，向量稠密。我们经常说的词向量，其实就是分布式表示方法的一种！相对One-Hot来说，分布式的容量空间要大得多。

很多模型都能通过训练，从而得出分布式的词向量。如：

- Skip-Gram
- CBOW
- Glove
- RNN
- LSTM
- MF

这些模型的超参数中，都有一个重要的参数可以自行定义，那就是词向量的大小。通常可以设置为50-300。在理想的情况下，通过训练得出的词向量在某种意义上是可以代表出一个词真正的意思。我们把一些高纬度的词向量，经过T-SNE算法降至二维后，通过作图可以发现，词向量可以在一定程度上把不同的词语区分开来：

![微信截图_20200530160831.png](https://i.loli.net/2020/05/31/scdZGlV1hHB4PT5.png)

### 倒排表

在基于检索的问答系统中，我们得到一个输入的句子后，一般的做法是直接在问答库中搜索，哪个句子与输入句子的相似度最高，从而输出对应的回答。这个做法的算法复杂度是 $O(N) * 计算相似度(S,Q)$。这个复杂度看起来不大，其实还是非常庞大的，因为我们的问答库有时候是非常庞大的。$O(N)$ 的线性搜索时间太过浪费时间。

为了解决这个问题，可以使用倒排表计算。

在我们拿到问答库以后，我们先做出词典，然后计算出每个词语出现在哪个文档中。每个词会对应有不同的文档序号，把这个对应关系存在表中。对于一个输入句子，先做分词，然后根据之前存好的表，可以知道分词出现哪些文档中，这样一来，就能快速定位到相应的文档中。

![微信截图_20200530161955.png](https://i.loli.net/2020/05/31/DkQAaFbe1I7f9lO.png)