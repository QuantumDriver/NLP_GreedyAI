{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编辑距离的计算\n",
    "编辑距离可以用来计算两个字符串的相似度，它的应用场景很多，其中之一是拼写纠正（spell correction）。 编辑距离的定义是给定两个字符串str1和str2, 我们要计算通过最少多少代价cost可以把str1转换成str2. \n",
    "\n",
    "举个例子：\n",
    "\n",
    "输入:   str1 = \"geek\", str2 = \"gesek\"\n",
    "输出:  1\n",
    "插入 's'即可以把str1转换成str2\n",
    "\n",
    "输入:   str1 = \"cat\", str2 = \"cut\"\n",
    "输出:  1\n",
    "用u去替换a即可以得到str2\n",
    "\n",
    "输入:   str1 = \"sunday\", str2 = \"saturday\"\n",
    "输出:  3\n",
    "\n",
    "我们假定有三个不同的操作： 1. 插入新的字符   2. 替换字符   3. 删除一个字符。 每一个操作的代价为1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于动态规划的解法\n",
    "def edit_dist(str1, str2):\n",
    "    # m，n分别字符串str1和str2的长度\n",
    "    m, n = len(str1), len(str2)\n",
    "    # 构建二位数组来存储子问题（sub-problem)的答案 \n",
    "    dp = [[0 for x in range(n+1)] for x in range(m+1)] \n",
    "    # 利用动态规划算法，填充数组\n",
    "    for i in range(m+1): \n",
    "        for j in range(n+1):  \n",
    "    # 假设第一个字符串为空，则转换的代价为j (j次的插入)\n",
    "            if i == 0: \n",
    "                dp[i][j] = j    \n",
    "    # 同样的，假设第二个字符串为空，则转换的代价为i (i次的插入)\n",
    "            elif j == 0:\n",
    "                dp[i][j] = i    \n",
    "    # 如果最后一个字符相等，就不会产生代价\n",
    "            elif str1[i-1] == str2[j-1]: \n",
    "                dp[i][j] = dp[i-1][j-1] \n",
    "    # 如果最后一个字符不一样，则考虑多种可能性，并且选择其中最小的值\n",
    "            else: \n",
    "                dp[i][j] = 1 + min(dp[i][j-1],        # Insert \n",
    "                                   dp[i-1][j],        # Remove \n",
    "                                   dp[i-1][j-1])      # Replace \n",
    "    return dp[m][n] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自己的写的编辑距离\n",
    "def edit_dis(s1,s2):\n",
    "    m, n = len(s1), len(s2)\n",
    "    dp = [[0] * (n+1) for _ in range(m+1)]\n",
    "    for i in range(m+1):\n",
    "        dp[i][0] = i\n",
    "    for i in range(n+1):\n",
    "        dp[0][i] = i\n",
    "    for i in range(1,m+1):\n",
    "        for j in range(1,n+1):\n",
    "            if s1[i-1] == s2[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1]\n",
    "            else:\n",
    "                dp[i][j] = min(dp[i-1][j-1],dp[i-1][j],dp[i][j-1]) + 1\n",
    "    return dp[-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit_dist('apple','appl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit_dis('like','love')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成指定编辑距离的单词\n",
    "给定一个单词，我们也可以生成编辑距离为K的单词列表。 比如给定 str=\"apple\"，K=1, 可以生成“appl”, \"appla\", \"pple\"...等\n",
    "下面看怎么生成这些单词。 还是用英文的例子来说明。 仍然假设有三种操作 - 插入，删除，替换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 'apple'),\n",
       " ('a', 'pple'),\n",
       " ('ap', 'ple'),\n",
       " ('app', 'le'),\n",
       " ('appl', 'e'),\n",
       " ('apple', '')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 先把单词切分成可能的结果\n",
    "str = 'apple'\n",
    "splits = [(str[:i], str[i:])for i in range(len(str)+1)]\n",
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aapple', 'bapple', 'capple', 'dapple', 'eapple', 'fapple', 'gapple', 'happle', 'iapple', 'japple', 'kapple', 'lapple', 'mapple', 'napple', 'oapple', 'papple', 'qapple', 'rapple', 'sapple', 'tapple', 'uapple', 'vapple', 'wapple', 'xapple', 'yapple', 'zapple', 'aapple', 'abpple', 'acpple', 'adpple', 'aepple', 'afpple', 'agpple', 'ahpple', 'aipple', 'ajpple', 'akpple', 'alpple', 'ampple', 'anpple', 'aopple', 'appple', 'aqpple', 'arpple', 'aspple', 'atpple', 'aupple', 'avpple', 'awpple', 'axpple', 'aypple', 'azpple', 'apaple', 'apbple', 'apcple', 'apdple', 'apeple', 'apfple', 'apgple', 'aphple', 'apiple', 'apjple', 'apkple', 'aplple', 'apmple', 'apnple', 'apople', 'appple', 'apqple', 'aprple', 'apsple', 'aptple', 'apuple', 'apvple', 'apwple', 'apxple', 'apyple', 'apzple', 'appale', 'appble', 'appcle', 'appdle', 'appele', 'appfle', 'appgle', 'apphle', 'appile', 'appjle', 'appkle', 'applle', 'appmle', 'appnle', 'appole', 'appple', 'appqle', 'apprle', 'appsle', 'apptle', 'appule', 'appvle', 'appwle', 'appxle', 'appyle', 'appzle', 'applae', 'applbe', 'applce', 'applde', 'applee', 'applfe', 'applge', 'applhe', 'applie', 'applje', 'applke', 'applle', 'applme', 'applne', 'apploe', 'applpe', 'applqe', 'applre', 'applse', 'applte', 'applue', 'applve', 'applwe', 'applxe', 'applye', 'applze', 'applea', 'appleb', 'applec', 'appled', 'applee', 'applef', 'appleg', 'appleh', 'applei', 'applej', 'applek', 'applel', 'applem', 'applen', 'appleo', 'applep', 'appleq', 'appler', 'apples', 'applet', 'appleu', 'applev', 'applew', 'applex', 'appley', 'applez']\n",
      "Length:  156\n"
     ]
    }
   ],
   "source": [
    "# 可以观察插入操作的所有可能结果\n",
    "letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "inserts = [L + c + R for L, R in splits for c in letters]\n",
    "print(inserts)\n",
    "print('Length: ',len(inserts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281\n"
     ]
    }
   ],
   "source": [
    "def generate_edit_one(str):\n",
    "    \"\"\"\n",
    "    给定一个字符串，生成编辑距离为1的字符串列表。\n",
    "    \"\"\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits = [(str[:i], str[i:])for i in range(len(str)+1)]\n",
    "    inserts = [L + c + R for L, R in splits for c in letters]\n",
    "    deletes = [L + R[1:] for L, R in splits if R]\n",
    "    replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
    "    \n",
    "    #return set(splits)\n",
    "    return set(inserts + deletes + replaces)\n",
    "\n",
    "print (len(generate_edit_one(\"apple\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86524\n"
     ]
    }
   ],
   "source": [
    "def generate_edit_two(str):\n",
    "    \"\"\"\n",
    "    给定一个字符串，生成编辑距离不大于2的字符串\n",
    "    \"\"\"\n",
    "    # 直接对一个编辑距离单位内的字符串再进行一次生成操作\n",
    "    return [e2 for e1 in generate_edit_one(str) for e2 in generate_edit_one(e1)]\n",
    "\n",
    "print (len(generate_edit_two(\"apple\"))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于结巴（jieba）的分词。 Jieba是最常用的中文分词工具~ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\13974\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.726 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Mode: 贪心/ 心学/ 学院/ 专注/ 于/ 人工/ 人工智能/ 智能/ 教育\n",
      "Default Mode: 贪心学院/ 专注/ 于/ 人工智能/ 教育\n"
     ]
    }
   ],
   "source": [
    "# encoding=utf-8\n",
    "import jieba\n",
    "\n",
    "# 基于jieba的分词\n",
    "# 全模型下返回所有可能的结果\n",
    "seg_list = jieba.cut(\"贪心学院专注于人工智能教育\", cut_all=True)\n",
    "print(\"Default Mode: \" + \"/ \".join(seg_list))  \n",
    "\n",
    "jieba.add_word(\"贪心学院\")\n",
    "seg_list = jieba.cut(\"贪心学院专注于人工智能教育\", cut_all=False)\n",
    "print(\"Default Mode: \" + \"/ \".join(seg_list)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Tokenizer.cut at 0x000001D8C61B8DE0> <generator object Tokenizer.cut at 0x000001D8C6194228>\n",
      "今天|晚上|想|吃|黄焖|鸡\n",
      "今天|中午|大家|都|想|吃|韩国|料理\n"
     ]
    }
   ],
   "source": [
    "s1 = '今天晚上想吃黄焖鸡'\n",
    "s2 = '今天中午大家都想吃韩国料理'\n",
    "js1 = jieba.cut(s1)\n",
    "js2 = jieba.cut(s2)\n",
    "# 分词后的结果是一个生成器\n",
    "print(js1,js2)\n",
    "print('|'.join(js1))\n",
    "print('|'.join(js2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 判断一句话是否能够切分（被字典）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手写一个分词判断工具\n",
    "dic = set([\"贪心科技\", \"人工智能\", \"教育\", \"在线\", \"专注于\"])\n",
    "def word_break(str):\n",
    "    could_break = [False] * (len(str) + 1)\n",
    "    could_break[0] = True\n",
    "    for i in range(1, len(could_break)):\n",
    "        for j in range(0, i):\n",
    "            # could_break[j]是当前词起始位置，必须要满足True\n",
    "            if str[j:i] in dic and could_break[j] == True:\n",
    "                could_break[i] = True\n",
    "    print(could_break)\n",
    "    return could_break[len(str)] == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, False, False, False, True, False, True, False, True]\n",
      "[True, False, True, False, True, False]\n",
      "[True]\n",
      "[True, False, True, False, True, False, False, False, True]\n"
     ]
    }
   ],
   "source": [
    "assert word_break(\"贪心科技在线教育\")==True\n",
    "# '是'不在词典中，因此无法被切分\n",
    "assert word_break(\"在线教育是\")==False\n",
    "assert word_break(\"\")==True\n",
    "assert word_break(\"在线教育人工智能\")==True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 思考题：给定一个词典和一个字符串，能不能返回所有有效的分割？ （valid segmentation) \n",
    "比如给定词典：dic = set([\"贪心科技\", \"人工智能\", \"教育\", \"在线\", \"专注于\"， “贪心”])\n",
    "和一个字符串 = “贪心科技专注于人工智能”\n",
    "\n",
    "输出为： \n",
    "“贪心” “科技” “专注于” “人工智能”\n",
    "\"贪心科技\" “专注于” “人工智能”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_possible_segmentations(str):\n",
    "    segs = []\n",
    "    dic = set([\"贪心科技\", \"人工智能\", \"教育\", \"在线\", \"专注于\",\"贪心\"])\n",
    "    n = len(str)\n",
    "    Flag = [False] * (n+1)\n",
    "    Flag[0] = True\n",
    "    for i in range(1,n+1):\n",
    "        for j in range(i):\n",
    "            if str[j:i] in dic and Flag[j]:\n",
    "                print(str[j:i])\n",
    "                Flag[i] = True\n",
    "                segs.append(str[j:i])\n",
    "    return segs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmm(source, words_dict):\n",
    "    \"\"\"\n",
    "    前向最大匹配算法\n",
    "    \"\"\"\n",
    "    len_source = len(source)  # 原句长度\n",
    "    index = 0\n",
    "    words = []  # 分词后句子每个词的列表\n",
    "\n",
    "    while index < len_source:  # 如果下标未超过句子长度\n",
    "        match = False\n",
    "        for i in range(window_size, 0, -1):\n",
    "            sub_str = source[index: index+i]\n",
    "            if sub_str in words_dict:\n",
    "                match = True\n",
    "                words.append(sub_str)\n",
    "                index += i\n",
    "                break\n",
    "        if not match:\n",
    "            words.append(source[index])\n",
    "            index += 1\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bmm(source, word_dict):\n",
    "    \"\"\"\n",
    "    后向最大匹配算法\n",
    "    \"\"\"\n",
    "    len_source = len(source)  # 原句长度\n",
    "    index = len_source\n",
    "    words = []  # 分词后句子每个词的列表\n",
    "\n",
    "    while index > 0:\n",
    "        match = False\n",
    "        for i in range(window_size, 0, -1):\n",
    "            sub_str = source[index-i: index]\n",
    "            if sub_str in word_dict:\n",
    "                match = True\n",
    "                words.append(sub_str)\n",
    "                index -= i\n",
    "                break\n",
    "        if not match:\n",
    "            words.append(source[index-1])\n",
    "            index -= 1\n",
    "    words.reverse()  # 得到的列表倒序\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bi_mm(source, words_dict):\n",
    "    \"\"\"\n",
    "    双向最大匹配算法\n",
    "    \"\"\"\n",
    "    forward = fmm(source, words_dict)\n",
    "    backward = bmm(source, words_dict)\n",
    "    # 正反向分词结果\n",
    "    print(\"FMM: \", forward)\n",
    "    print(\"BMM: \", backward)\n",
    "    # 单字词个数\n",
    "    f_single_word = 0\n",
    "    b_single_word = 0\n",
    "    # 总词数\n",
    "    tot_fmm = len(forward)\n",
    "    tot_bmm = len(backward)\n",
    "    # 非字典词数\n",
    "    oov_fmm = 0\n",
    "    oov_bmm = 0\n",
    "    # 罚分，罚分值越低越好\n",
    "    score_fmm = 0\n",
    "    score_bmm = 0\n",
    "    # 如果正向和反向结果一样，返回任意一个\n",
    "    if forward == backward:\n",
    "        return backward\n",
    "    # print(backward)\n",
    "    else:  # 分词结果不同，返回单字数、非字典词、总词数少的那一个\n",
    "        for each in forward:\n",
    "            if len(each) == 1:\n",
    "                f_single_word += 1\n",
    "        for each in backward:\n",
    "            if len(each) == 1:\n",
    "                b_single_word += 1\n",
    "        for each in forward:\n",
    "            if each not in words_dict:\n",
    "                oov_fmm += 1\n",
    "        for each in backward:\n",
    "            if each not in backward:\n",
    "                oov_bmm += 1\n",
    "        # 可以根据实际情况调整惩罚分值\n",
    "        # 这里都罚分都为1分\n",
    "        # 非字典词越少越好\n",
    "        if oov_fmm > oov_bmm:\n",
    "            score_bmm += 1\n",
    "        elif oov_fmm < oov_bmm:\n",
    "            score_fmm += 1\n",
    "        # 总词数越少越好\n",
    "        if tot_fmm > tot_bmm:\n",
    "            score_bmm += 1\n",
    "        elif tot_fmm < tot_bmm:\n",
    "            score_fmm += 1\n",
    "        # 单字词越少越好\n",
    "        if f_single_word > b_single_word:\n",
    "            score_bmm += 1\n",
    "        elif f_single_word < b_single_word:\n",
    "            score_fmm += 1\n",
    "\n",
    "        # 返回罚分少的那个\n",
    "        if score_fmm < score_bmm:\n",
    "            return forward\n",
    "        else:\n",
    "            return backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = set([\"贪心科技\", \"人工智能\", \"教育\", \"在线\", \"专注于\",\"贪心\"])\n",
    "window_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "贪心\n",
      "贪心科技\n",
      "专注于\n",
      "人工智能\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['贪心', '贪心科技', '专注于', '人工智能']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = '贪心科技专注于人工智能'\n",
    "all_possible_segmentations(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['贪心科技', '专注于', '人工智能']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmm(s,dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['贪心科技', '专注于', '人工智能']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bmm(s,dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FMM:  ['贪心科技', '专注于', '人工智能']\n",
      "BMM:  ['贪心科技', '专注于', '人工智能']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['贪心科技', '专注于', '人工智能']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_mm(s,dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 停用词过滤\n",
    "出现频率特别高的和频率特别低的词对于文本分析帮助不大，一般在预处理阶段会过滤掉。 \n",
    "在英文里，经典的停用词为 “The”, \"an\".... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'are', 'students']\n"
     ]
    }
   ],
   "source": [
    "# 方法1： 自己建立一个停用词词典\n",
    "stop_words = [\"the\", \"an\", \"is\", \"there\"]\n",
    "# 在使用时： 假设 word_list包含了文本里的单词\n",
    "word_list = [\"we\", \"are\", \"the\", \"students\"]\n",
    "filtered_words = [word for word in word_list if word not in stop_words]\n",
    "print (filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法2：直接利用别人已经构建好的停用词库\n",
    "from nltk.corpus import stopwords\n",
    "cachedStopWords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(cachedStopWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 波特词干法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caress fli die mule deni die agre own humbl size meet state siez item sensat tradit refer colon plot\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "test_strs = ['caresses', 'flies', 'dies', 'mules', 'denied',\n",
    "         'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "         'meeting', 'stating', 'siezing', 'itemization',\n",
    "         'sensational', 'traditional', 'reference', 'colonizer',\n",
    "         'plotted']\n",
    "\n",
    "singles = [stemmer.stem(word) for word in test_strs]\n",
    "print(' '.join(singles))  # doctest: +NORMALIZE_WHITESPACE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词袋向量： 把文本转换成向量 。 只有向量才能作为模型的输入。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法1： 词袋模型（按照词语出现的个数）\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = [\n",
    "     'He is going from Beijing to Shanghai.',\n",
    "     'He denied my request, but he actually lied.',\n",
    "     'Mike lost the phone, and phone was in the car.',\n",
    "]\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 基于count计数的ont-hot向量，词典按照字母表排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 0]\n",
      " [1 0 0 1 0 1 0 0 2 0 0 1 0 0 1 0 1 0 0 0 0]\n",
      " [0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 2 0 0 2 0 1]]\n",
      "['actually', 'and', 'beijing', 'but', 'car', 'denied', 'from', 'going', 'he', 'in', 'is', 'lied', 'lost', 'mike', 'my', 'phone', 'request', 'shanghai', 'the', 'to', 'was']\n"
     ]
    }
   ],
   "source": [
    "print (X.toarray())\n",
    "print (vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn的tf-idf计算首先去掉了平滑操作，其次还用了Normalize的方法，所以结果和用通常所见的式子的计算结算不一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法2：词袋模型（tf-idf方法）\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(smooth_idf=False)\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.39379499 0.         0.         0.\n",
      "  0.39379499 0.39379499 0.26372909 0.         0.39379499 0.\n",
      "  0.         0.         0.         0.         0.         0.39379499\n",
      "  0.         0.39379499 0.        ]\n",
      " [0.35819397 0.         0.         0.35819397 0.         0.35819397\n",
      "  0.         0.         0.47977335 0.         0.         0.35819397\n",
      "  0.         0.         0.35819397 0.         0.35819397 0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.26726124 0.         0.         0.26726124 0.\n",
      "  0.         0.         0.         0.26726124 0.         0.\n",
      "  0.26726124 0.26726124 0.         0.53452248 0.         0.\n",
      "  0.53452248 0.         0.26726124]]\n",
      "['actually', 'and', 'beijing', 'but', 'car', 'denied', 'from', 'going', 'he', 'in', 'is', 'lied', 'lost', 'mike', 'my', 'phone', 'request', 'shanghai', 'the', 'to', 'was']\n"
     ]
    }
   ],
   "source": [
    "print (X.toarray())\n",
    "print (vectorizer.get_feature_names())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
