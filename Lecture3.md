### 课程内容

视频编号：P20 - P28

1. 文本处理的流程
2. 分词
3. 拼写纠错
4. Filtering Words
5. Words Normalization

### 文本处理的流程

|         | step1    |    step2     | step3                  | step4           | step5              | step6      |
| ------- | -------- | :----------: | ---------------------- | --------------- | ------------------ | ---------- |
| name    | Raw Data | Segmentation | Clean                  | Normalization、 | Feature Extraction | Modeling   |
| Details | 文本     |  英文：容易  | 特殊符号，标签，停用词 | Stemming        | TF-IDF             | 相似度计算 |
|         | 新闻     |  中文：困难  | 大小写                 | Lemmatization   | Word2Vec           | 分类算法   |

通常在结束以上6个步骤以后还会进行一定的评估。

可以发现，NLP项目在预处理上会进行较多的步骤，相对CV项目来说NLP会更繁琐一些。

### 分词

常用的分词工具有：

- jieba
- SnowNLP
- LTP
- HanNLP

分词的底层算法常见的有两种：

1. 基于匹配的分词 (规则)：
    - 前向最大匹配
    - 后向最大匹配
    - 双向最大匹配
2. 基于概率的分词 (语义)：
    - Language Model 语言模型 (n-gram)
    - HMM (隐马尔可夫模型), CRF (条件随机场模型)

#### 前向最大匹配算法 Forward-max Matching

对于一个语句S，设定一个滑动窗口，长度为n (n通常是根据经验来设置的，一般是5-10)。观察S[0:n]这个字符串是否在词典中，若在，则取出，形成一个分词，若不在则对n进行一个单位的递减直到S[0:i]出现在词典中。然后下一次对S[i:i+n]重复上一步的操作，直到所有的词都被分割好。

#### 后向最大匹配算法 Backward-max Matching

与前向最大匹配的匹配方法类似，不过是后向匹配，即从S[-n:]进行匹配，-n会进行递增，直到S[i:]匹配成功。然后下一次匹配就会从S[-i-n:i]开始匹配，重复之前的方法。

#### 匹配分词的缺点

1. 不能对词进行进一步的细分
2. 只能取得局部最优
3. 复杂度较高，效率很低
4. **最重要的一点是，没有考虑上下文和语义！**

#### 基于语义的分词方法 Incorporate Semantic

假设存在一个语言模型LM，可以把我们分词的结果1,2,3分别进行计算，可以算出这个句子合理性的概率，如0.4,0.5,0.6。那么我们就能根据这个概率取一个最优解。

但这方法有一个缺点是，我们要枚举所有可能的分词结果，这就会使得算法的复杂度很高，效率低下！要改进这个方法，可以把**分词结果枚举**与**句子概率计算**这两个步骤进行合并，替换成一个图的最短路径问题！

#### Viterbi算法

假设有一个句子S，我们找到了句子S所有可能出现的分词，将其组合成一个词典，通过语言模型计算出词典内每一个词的概率，再对概率取一个负的log。即 $-\log(P)$，这样我们的优化目标就从求最大值变成了求最小值。观察下面的这个例子：

![微信截图_20200528185209.png](https://i.loli.net/2020/05/29/FQZkYPxjMhfTXGy.png)

我们把一句话看成一个字一个字的组合，字与字之间由一个节点连接，每个字都是一条边，这个边的值或者说权重就是这个字的对数化以后的概率值取负数，如果一个字在词典中不存在，那么我们可以定义一个专门的小概率值，专门赋值给这些不在词典中的字。

接着我们考虑从1到8中，两个字两个字互相连接是否存在可能，如果两个字在字典中出现，那么可以互相连接，形成一条新的边，遍历结束之后再考虑三个字，四个字...直到词典中最大的词的长度。

当我们把节点之间的边构造完成之后，我们要考虑的问题是，如何求从第一个节点到最后一个节点最短的累加权重，如果把边的权重看成是长度的话，那么我们要求的就是从1到8的最短路径。这显然可以用动态规划的方式进行解决，其递归式可以写作 

$f(node) = min(f(sub\_node)+sub\_node.value$  $for$ $sub\_node$ $in$ $linked\_node)$

$f(node)$ 表示从起始点到节点 $node$ 的最小值，$sub\_node$ 是连接 $node$ 的所有节点，统一称为 $linked\_node$，那么 $sub\_node.value$ 则是一条边的值。通过这个通式，我们可以维护两个数组，一个存放历史上计算出的所有 $f(node)$ 的值，另一个列表则存放到此节点最小值的那条边，用index表示。

这样，我们就能找到最短路径和最小值是多少了。

#### 分词总结

- 分词的方法大致有两种：一是基于匹配，二是基于概率
- 分词可以认为是已经解决的问题！

### 拼写纠错

Spell Correction通常有两个应用场景：一是纠正错别字，二是纠正不合适的字。纠错时，用编辑距离计算错误词和正确词的距离单位，距离越小的越有可能是期望达到的词。

![微信截图_20200528191615.png](https://i.loli.net/2020/05/29/z9x4Aq8Plm3HkOu.png)

以上图为例，假设用户输入了一个错误词，我们将会计算输入词与候选词的编辑距离，然后选出最优的词。然而，编辑距离的计算复杂度较高，是DP的一种，复杂度为 $O(M*N)$，所以我们需要找到另一种效率更高的方法。

- Alternative Way

根据用户的输入S，我们可以生成与S编辑距离为1或2的所有可能的字符串，然后再对这些字符串进行过滤，最后返回最优解。因为在实际生活中，我们的输入往往就是1到2编辑距离单位的误差。

假设根据输入，我们生成了 $10^4 \sim 10^5$ 个候选字符串，如何求解最优的那个字符串呢？

现在，问题就变成了给定S(string)，Can(Candidates-候选字符串)，寻找最优的字符串C，所以可以把这个问题转化为数学式：
$$
\hat{c}=\arg\min _{c \in Can}p(c| s)=\arg\min _{c \in Can}\frac{p(s | c) p(c)}{p(s)}
$$
由于S是给定的，那么 $p(s)$ 是一个常数，所以有
$$
\hat{c} = \arg\min _{c \in Can}{p(s | c) · p(c)}
$$
总结为：
$$
\begin{aligned}
\hat{c} &=\operatorname{argmax}_{c \in \text {candidates}} p(c | s) \\
&=\operatorname{argmax}_{c \in \text {candidates}}\frac{p({s} | \mathrm{c}) * p(c)}{p(s)} \\
&=\operatorname{argmax}_{c \in \text {candidates}} p({s} | \mathrm{c}) * p(c)
\end{aligned}
$$
根据历史数据，计算出 $p(s|c)$ 和 $p(c)$ 并不难，简单的统计就能得到答案。

### Filtering Words

过滤词比较简单，其实就是把停用词和低频词过滤掉，比如说 a , an , the 这种词。我们可以直接选取别人词库中的停用词来为自己的文本做过滤，也可以根据自己的特点，加入或减少某些停用词，这需要根据自己文本的场景去进行设定。

其实词的过滤类似于特征选择，把没有用的特征(词)过滤掉。

### Words Normalization

词语的规范化一般有两种方式：Stemming(词干提取) or Lemmatization(词形还原)

Stemming就是相当于把fly, flies归结到一个词，如fli，这个fli很有可能是不在词典中的。

而Lemmatization则是相当于把went, go, going归结到go，它是一定会在词典中的。

实现Stemming的一种方式是 Porter Stemmer(波特词干算法)，这个方法相当于是靠规则和语言学家的知识这个两个支柱去实现的。如下图所示：

[![20200528195252.png](https://i.postimg.cc/sDVyYPJw/20200528195252.png)](https://postimg.cc/qgZ903S3)

